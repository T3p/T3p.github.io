---
layout: post
title: SVRPG is Provably Better than REINFORCE
---

Pan Xu, Felicia Gao and Quanquan Gu from UCLA proved a **convergence rate** of \\(O(\epsilon^{-5/3})\\) for our **Stochastic Variance-Reduced Policy Gradient (SVRPG)** algorithm.  

Our [original paper][1] \[1\] only proved asymptotic convergence of SVRPG.  

The paper by Xu and colleagues \[2\] recently appeared in the [proceedings of UAI 2019][2].  
The new convergence rate improves over the \\(O(\epsilon^{-2})\\) rate of REINFORCE, showing that SVRPG is indeed **more sample efficient**.
This is the number of sample trajectories needed to achieve a squared gradient norm smaller than \\(\epsilon\\).  
The authors also provide new insights on how to select the **hyper-parameters** of SVRPG, which may be useful for future practical implementations.

Recently, Z. Shen et al. proved a convergence rate of \\(O(\epsilon^{-3/2})\\) for their novel Hessian Aided Policy Gradient Algorithm \[3\], which is based on a different variance-reduction technique.  
This makes it the policy gradient algorithm with the best known convergence rate.  
I wonder if the same rate can be achieved by SVRPG or some improvement of it.

&nbsp;
&nbsp;

\[1\] *Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli; "Stochastic Variance-Reduced Policy Gradient"; Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4026-4035, 2018.*

\[2\] *Pan Xu, Felicia Gao, Quanquan Gu; "An Improved Convergence Analysis of Stochastic Variance-Reduced Policy Gradient"; Proceedings of the 35th International Conference on Uncertainty in Artificial Intelligence, Tel Aviv, Israel, 2019.*

\[3\] *Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, Chao Mi ; "Hessian Aided Policy Gradient"; 
Proceedings of the 36th International Conference on Machine Learning, PMLR 97:5729-5738, 2019.*

[1]:{{ site.url }}/ICML/
[2]:http://auai.org/uai2019/accepted.php
