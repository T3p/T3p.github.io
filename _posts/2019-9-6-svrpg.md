---
layout: post
title: SVRPG is provably better than REINFORCE
---

Pan Xu, Felicia Gao and Quanquan Gu from UCLA proved a **convergence rate** of \\(O(\epsilon^{-5/3})\\) for our **Stochastic Variance-Reduced Policy Gradient (SRVPG)** algorithm.
This is the number of sample trajectories needed to achieve a squared gradient norm smaller than \\(\epsilon\\).

The new result improves over the \\(O(\epsilon^{-2})\\) rate of REINFORCE, showing that SVRPG is indeed **more sample efficient**. Our original paper \[1\] only proved convergence of SVRPG.

The paper by Xu and colleagues \[2\] recently appeared in the proceedings of [UAI 2019][2]. The authors also provide new insights on how to select the **hyper-parameters** of SVRPG, which may be useful for future practical implementations.

&nbsp;
&nbsp;

\[1\] Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli; "Stochastic Variance-Reduced Policy Gradient"; Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4026-4035, 2018. \[[bibtex][3]\]\[[more][1]\]

\[2\] Pan Xu, Felicia Gao, Quanquan Gu; "An Improved Convergence Analysis of Stochastic Variance-Reduced Policy Gradient"; UAI 2019" \[[pdf][4]\]

[1]:{{ site.url }}/ICML/
[2]:http://auai.org/uai2019/
[3]:{{ site.url }}/download/pmlr-v80-papini18a.bib
[4]:http://auai.org/uai2019/proceedings/papers/191.pdf
