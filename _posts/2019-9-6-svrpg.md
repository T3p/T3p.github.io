---
layout: post
title: SVRPG is provably better than REINFORCE
---

Pan Xu, Felicia Gao and Quanquan Gu from UCLA proved a **convergence rate** of \\(O(\epsilon^{-5/3})\\) for our **Stochastic Variance-Reduced Policy Gradient (SRVPG)** algorithm.  

Our [original paper][1] \[2\] only proved asymptotic convergence of SVRPG.  

The paper by Xu and colleagues \[1\] recently appeared in the [proceedings of UAI 2019][2].  
The new convergence rate improves over the \\(O(\epsilon^{-2})\\) rate of REINFORCE, showing that SVRPG is indeed **more sample efficient**.
This is the number of sample trajectories needed to achieve a squared gradient norm smaller than \\(\epsilon\\).  
The authors also provide new insights on how to select the **hyper-parameters** of SVRPG, which may be useful for future practical implementations.

&nbsp;
&nbsp;

\[1\] *Pan Xu, Felicia Gao, Quanquan Gu; "An Improved Convergence Analysis of Stochastic Variance-Reduced Policy Gradient"; Proceedings of the 35th International Conference on Uncertainty in Artificial Intelligence, Tel Aviv, Israel, 2019.*

\[2\] *Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli; "Stochastic Variance-Reduced Policy Gradient"; Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4026-4035, 2018.*


[1]:{{ site.url }}/ICML/
[2]:http://auai.org/uai2019/accepted.php
