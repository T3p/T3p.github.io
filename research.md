---
layout: page
title: Research
permalink: /research/
---
See my *researchgate* profile [here][1]

### Research interests
Reinforcement Learning, Machine Learning, AI Safety


### Conference Papers 
* Lorenzo Bisi, Luca Sabbioni, Edoardo Vittori, **Matteo Papini**, Marcello Restelli, "Risk-Averse Trust Region Optimization for Reward-Volatility Reduction", **IJCAI 2020**; Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence
Special Track on AI in FinTech. Pages 4583-4589, 2020. \[[pdf][15]\]\[[bibtex][16]\]

* **Matteo Papini**, Andrea Battistello, Marcello Restelli, "Balancing Learning Speed and Stability in Policy Gradient via Adaptive Exploration", **AISTATS 2020**; 
Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, PMLR 108:1188-1199, 2020. \[[bibtex][11]\]\[[more][12]\]

* **Matteo Papini**, Alberto Maria Metelli, Lorenzo Lupo, Marcello Restelli, "Optimistic Policy Optimization via Multiple Importance Sampling", **ICML 2019**; Proceedings of the 36th International Conference on Machine Learning, PMLR 97:4989-4999, 2019. \[[bibtex][9]\]\[[more][8]\] 

* Alberto Metelli, **Matteo Papini**, Francesco Faccio, Marcello Restelli, "Policy Optimization via Importance Sampling", **NeurIPS 2018**; In Advances in Neural Information Processing Systems, pp. 5442-5454. 2018 \[[bibtex][10]\]\[[more][7]\]
 
*  **Matteo Papini**, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, Marcello Restelli, "Stochastic Variance-Reduced Policy Gradient", **ICML 2018**; Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4026-4035, 2018. \[[bibtex][3]\]\[[more][5]\] 

* **Matteo Papini**, Matteo Pirotta, Marcello Restelli, "Adaptive batch size for safe policy gradients", **NeurIPS 2017**; Advances in Neural Information Processing Systems, pp. 3591-3600, 2017 \[[bibtex][2]\]\[[more][4]\] 

### Journal Papers
* Alberto Maria Metelli, **Matteo Papini**, Nico Montali, Marcello Restelli, "Importance Sampling Techniques for Policy Optimization", Journal of Machine Learning Research (**JMLR**) 21(141):1âˆ’75, 2020 \[[bibtex][13]\] \[[more][14]\]

### Preprints
* **Matteo Papini**, Matteo Pirotta, Marcello Restelli, "Smoothing Policies and Safe Policy Gradients"; \[[arXiv][17]\]

### Workshop Papers 
* Matteo Papini, Andrea Battistello, Marcello Restelli; "Safely Exploring Policy Gradient"; 14th European Workshop on Reinforcement Learning, Lille, France, 2018 \[[more][6]\] 

![image-title-here](../images/pen.jpg){:class="img-responsive"}

[1]:https://www.researchgate.net/profile/Matteo_Papini
[2]:http://papers.nips.cc/paper/6950-adaptive-batch-size-for-safe-policy-gradients/bibtex 
[3]:{{ site.url }}/download/pmlr-v80-papini18a.bib 
[4]:{{ site.url }}/Poster/ 
[5]:{{ site.url }}/ICML/ 
[6]:{{ site.url }}/EWRL
[7]:{{ site.url }}/neurips18.md
[8]:{{ site.url }}/icml19/
[9]:{{ site.url }}/download/pmlr-v97-papini19a.bib
[10]:{{ site.url }}/download/policy-optimization-via-importance-sampling.bib
[11]:http://proceedings.mlr.press/v108/papini20a.html
[12]:{{ site.url }}/aistats/
[13]:http://jmlr.org/papers/v21/20-124.bib
[14]:{{ site.url }}/jmlr/
[15]:https://www.ijcai.org/proceedings/2020/0632.pdf
[16]:https://www.ijcai.org/proceedings/2020/bibtex/632
[17]:https://arxiv.org/abs/1905.03231
